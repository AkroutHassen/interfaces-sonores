# app.py
import streamlit as st
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
import tempfile
import os
import joblib
import pandas as pd
import math
from scipy.signal import find_peaks

# Configuration de la page
st.set_page_config(page_title="Analyseur Audio", layout="wide")
st.title("üéµ Analyseur Audio")

# ------------------------------
# Barre lat√©rale : T√©l√©chargement de fichier, lecteur audio et bouton de pr√©diction
# ------------------------------
st.sidebar.header("T√©l√©chargement et Contr√¥les")
uploaded_file = st.sidebar.file_uploader("T√©l√©chargez un fichier audio (MP3/WAV)", type=["wav", "mp3"])

# Barre lat√©rale : S√©lection de la dur√©e des segments
st.sidebar.subheader("Dur√©e des segments")
chunk_duration = st.sidebar.slider(
    "S√©lectionnez la dur√©e des segments (en secondes) :",
    min_value=1,
    max_value=30,
    value=10,  # Valeur par d√©faut
    step=1
)

# Initialisation du drapeau de pr√©diction
do_predictions = False

if uploaded_file is not None:
    # Sauvegarder temporairement le fichier t√©l√©charg√©
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3" if uploaded_file.type == "audio/mpeg" else ".wav") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name

    # Charger l'audio avec librosa
    y, sr = librosa.load(tmp_path, mono=False, sr=None)
    # Convertir en mono si n√©cessaire pour le traitement
    audio_mono = librosa.to_mono(y) if y.ndim > 1 else y

    # Lecteur audio dans la barre lat√©rale
    st.sidebar.audio(uploaded_file)
    
    # Bouton de pr√©diction dans la barre lat√©rale
    if st.sidebar.button("Faire des pr√©dictions"):
        do_predictions = True

    if st.sidebar.button("Visualisations"):
        do_predictions = False

# ------------------------------
# Fonctions auxiliaires
# ------------------------------
def analyze_audio(y, sr):
    """Extraire les caract√©ristiques audio d'une s√©rie temporelle et d'une fr√©quence d'√©chantillonnage."""
    if y.ndim > 1:
        y = np.mean(y, axis=0)

    features = {}
    features["length"] = len(y)
    chroma_hop_length = 512
    chromagram = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=chroma_hop_length)
    features["chroma_stft_mean"] = chromagram.mean()
    features["chroma_stft_var"] = chromagram.var()
    rms = librosa.feature.rms(y=y)
    features["rms_mean"] = rms.mean()
    features["rms_var"] = rms.var()
    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
    features["spectral_centroid_mean"] = spectral_centroid.mean()
    features["spectral_centroid_var"] = spectral_centroid.var()
    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)
    features["spectral_bandwidth_mean"] = spectral_bandwidth.mean()
    features["spectral_bandwidth_var"] = spectral_bandwidth.var()
    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
    features["rolloff_mean"] = rolloff.mean()
    features["rolloff_var"] = rolloff.var()
    zero_crossing_rate = librosa.feature.zero_crossing_rate(y=y)
    features["zero_crossing_rate_mean"] = zero_crossing_rate.mean()
    features["zero_crossing_rate_var"] = zero_crossing_rate.var()
    harmony, perceptr = librosa.effects.hpss(y=y)
    features["harmony_mean"] = harmony.mean()
    features["harmony_var"] = harmony.var()
    features["perceptr_mean"] = perceptr.mean()
    features["perceptr_var"] = perceptr.var()
    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
    features["tempo"] = float(tempo)
    num_mfcc = 20
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=num_mfcc)
    for i in range(num_mfcc):
        features[f"mfcc{i+1}_mean"] = mfcc[i].mean()
        features[f"mfcc{i+1}_var"] = mfcc[i].var()
    return features

# Continuez √† traduire les autres sections de votre code de mani√®re similaire.

def build_features_array(features):
    """Build a features array in the same order used during model training."""
    feature_list = [
        features["length"],
        features["chroma_stft_mean"],
        features["chroma_stft_var"],
        features["rms_mean"],
        features["rms_var"],
        features["spectral_centroid_mean"],
        features["spectral_centroid_var"],
        features["spectral_bandwidth_mean"],
        features["spectral_bandwidth_var"],
        features["rolloff_mean"],
        features["rolloff_var"],
        features["zero_crossing_rate_mean"],
        features["zero_crossing_rate_var"],
        features["harmony_mean"],
        features["harmony_var"],
        features["perceptr_mean"],
        features["perceptr_var"],
        features["tempo"],
    ]
    for i in range(20):
        feature_list.append(features[f"mfcc{i+1}_mean"])
        feature_list.append(features[f"mfcc{i+1}_var"])
    return np.array(feature_list).reshape(1, -1)

# ------------------------------
# Main Page: Visualizations & Data Display
# ------------------------------
if uploaded_file is not None:
    # Determine audio channel info
    channels = "Mono" if y.ndim == 1 else "Stereo"
    duration = librosa.get_duration(y=y, sr=sr)

    # Display Metadata
    # Affichage des m√©tadonn√©es
    st.subheader("üìã M√©tadonn√©es")
    st.write(f"**Nom du fichier :** {uploaded_file.name}")
    st.write(f"**Taille du fichier :** {uploaded_file.size/1024:.2f} KB")
    st.write(f"**Format :** {uploaded_file.type}")
    st.write(f"**Canaux :** {channels}")
    st.write(f"**Fr√©quence d'√©chantillonnage :** {sr} Hz")
    st.write(f"**Dur√©e :** {duration:.2f} secondes")
    st.write(f"**Nombre d'√©chantillons :** {len(y)}")

    # Full track feature analysis
    full_features = analyze_audio(y, sr)
    if(do_predictions == False):
        # Waveform Visualization
        st.subheader("üé® Visualizations")
        st.markdown("### Visualisation de la forme d'onde")
        st.markdown("""
        La forme d'onde montre comment l'amplitude du signal audio varie dans le temps.  
        L'axe X repr√©sente le temps (en secondes) et l'axe Y repr√©sente l'amplitude du signal.
        """)
        
        fig1, ax1 = plt.subplots(figsize=(10, 3))
        if y.ndim == 1:
            librosa.display.waveshow(y, sr=sr, ax=ax1)
        else:
            for i in range(y.shape[0]):
                librosa.display.waveshow(y[i], sr=sr, ax=ax1, alpha=0.5, label=f"Channel {i+1}")
            ax1.legend()
        ax1.set_title("Waveform")
        ax1.set_xlabel("Temp (s)")
        ax1.set_ylabel("Amplitude")
        st.pyplot(fig1)

        # Fourier Transform Visualization Title and Description
        st.markdown("### Visualisation de la Transform√©e de Fourier")
        st.markdown("""
        La Transform√©e de Fourier d√©compose le signal en ses composantes fr√©quentielles.  
        Ce graphique montre les fr√©quences pr√©sentes et leur intensit√©.
        """)
        if y.ndim == 2:
            y = np.mean(y, axis=0)

        # Compute the FFT
        n = len(y)
        fft = np.fft.fft(y)
        fft_mag = np.abs(fft)[:n // 2]  # Keep only positive frequencies
        freqs = np.fft.fftfreq(n, d=1/sr)[:n // 2]

        # Plot the FFT
        fig, ax = plt.subplots(figsize=(10, 4))
        ax.plot(freqs, fft_mag, color='c')
        ax.set_title("Fourier Transform (Frequency Domain)")
        ax.set_xlabel("Frequency (Hz)")
        ax.set_ylabel("Magnitude")
        ax.set_xlim(0, sr // 2)
        st.pyplot(fig)


        # Spectrogram Visualization
        # Spectrogram Visualization Title and Description
        st.markdown("### Visualisation du Spectrogramme")
        st.markdown("""
        Le spectrogramme repr√©sente le contenu fr√©quentiel du signal audio dans le temps.  
        L'axe X montre le temps, l'axe Y montre les fr√©quences, et l'intensit√© des couleurs repr√©sente l'amplitude.
        """)
        fig2, ax2 = plt.subplots(figsize=(10, 4))
        if y.ndim == 1:
            D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)
        else:
            y_mono = np.mean(y, axis=0)
            D = librosa.amplitude_to_db(np.abs(librosa.stft(y_mono)), ref=np.max)
        img = librosa.display.specshow(D, y_axis="log", x_axis="time", sr=sr, ax=ax2)
        fig2.colorbar(img, ax=ax2, format="%+2.0f dB")
        ax2.set_title("Spectrogram")
        ax2.set_xlabel("Temps (s)")
        st.pyplot(fig2)

        # Mel Spectrogram Visualization Title and Description
        st.markdown("### Visualisation du Spectrogramme Mel")
        st.markdown("""
        Le spectrogramme Mel utilise une √©chelle perceptuelle des fr√©quences, adapt√©e √† la perception humaine.  
        Il est utile pour l'analyse de la parole et de la musique.
        """)
        if y.ndim == 2:

            # Process the left and right channels separately
            S_left = librosa.feature.melspectrogram(y=y[0], sr=sr)  # Left channel (index 0)
            S_right = librosa.feature.melspectrogram(y=y[1], sr=sr)  # Right channel (index 1)

            # Convert both to decibels (log scale)
            S_left_db = librosa.power_to_db(S_left, ref=np.max)
            S_right_db = librosa.power_to_db(S_right, ref=np.max)

            # Create subplots for left and right channels
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

            # Left channel plot
            img_left = librosa.display.specshow(S_left_db, y_axis='mel', x_axis='time', sr=sr, ax=ax1, cmap="cool")
            ax1.set_title("Left Channel Mel Spectrogram", fontsize=14)
            ax1.set_ylabel("Mel")
            fig.colorbar(img_left, ax=ax1, format="%+2.0f dB")

            # Right channel plot
            img_right = librosa.display.specshow(S_right_db, y_axis='mel', x_axis='time', sr=sr, ax=ax2, cmap="cool")
            ax2.set_title("Right Channel Mel Spectrogram", fontsize=14)
            ax2.set_ylabel("Mel")
            fig.colorbar(img_right, ax=ax2, format="%+2.0f dB")

            # Display the plots in Streamlit
            st.pyplot(fig)

        else:
            # If mono audio (y is 1D), compute and display the spectrogram
            S = librosa.feature.melspectrogram(y=y, sr=sr)
            S_db = librosa.power_to_db(S, ref=np.max)

            # Plot the Mel Spectrogram
            fig, ax = plt.subplots(figsize=(10, 4))
            img = librosa.display.specshow(S_db, y_axis='mel', x_axis='time', sr=sr, ax=ax, cmap="cool")
            fig.colorbar(img, ax=ax, format="%+2.0f dB")
            ax.set_xlabel("Temps (s)")
            ax.set_ylabel("Mel")
            ax.set_title("Mel Spectrogram", fontsize=14)

            # Display the plot in Streamlit
            st.pyplot(fig)


        # Visualisation des Caract√©ristiques Chroma
        st.markdown("### Visualisation des Caract√©ristiques Chroma")
        st.markdown("""
        Les caract√©ristiques Chroma repr√©sentent les 12 classes de hauteur musicale (par exemple, Do, R√©, Mi, etc.).  
        Cette visualisation montre comment l'intensit√© de ces classes varie dans le temps.
        """)

        if y.ndim == 2:  # St√©r√©o
            # Calcul des caract√©ristiques Chroma pour les deux canaux
            chroma_left = librosa.feature.chroma_stft(y=y[0], sr=sr)
            chroma_right = librosa.feature.chroma_stft(y=y[1], sr=sr)

            # Cr√©ation des sous-graphiques pour les canaux gauche et droit
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

            # Canal gauche
            img_left = librosa.display.specshow(chroma_left, y_axis='chroma', x_axis='time', sr=sr, ax=ax1, cmap="cool")
            ax1.set_title("Caract√©ristiques Chroma - Canal Gauche", fontsize=14)
            ax1.set_xlabel("Temps (s)")
            ax1.set_ylabel("Classe de Hauteur (Do, R√©, Mi, etc.)")
            fig.colorbar(img_left, ax=ax1, format="%+2.0f dB")

            # Canal droit
            img_right = librosa.display.specshow(chroma_right, y_axis='chroma', x_axis='time', sr=sr, ax=ax2, cmap="cool")
            ax2.set_title("Caract√©ristiques Chroma - Canal Droit", fontsize=14)
            ax2.set_xlabel("Temps (s)")
            ax2.set_ylabel("Classe de Hauteur (Do, R√©, Mi, etc.)")
            fig.colorbar(img_right, ax=ax2, format="%+2.0f dB")

            # Affichage des graphiques dans Streamlit
            st.pyplot(fig)

        else:  # Mono
            # Calcul des caract√©ristiques Chroma pour l'audio mono
            chroma = librosa.feature.chroma_stft(y=y, sr=sr)

            # Cr√©ation du graphique
            fig, ax = plt.subplots(figsize=(10, 4))
            img = librosa.display.specshow(chroma, y_axis='chroma', x_axis='time', sr=sr, ax=ax, cmap="cool")
            ax.set_title("Caract√©ristiques Chroma", fontsize=14)
            ax.set_xlabel("Temps (s)")
            ax.set_ylabel("Classe de Hauteur (Do, R√©, Mi, etc.)")
            fig.colorbar(img, ax=ax, format="%+2.0f dB")

            # Affichage du graphique dans Streamlit
            st.pyplot(fig)

        # Zero-Crossing Rate Visualization Title and Description
        st.markdown("### Visualisation du Taux de Passage par Z√©ro")
        st.markdown("""
        Le taux de passage par z√©ro mesure la fr√©quence √† laquelle le signal audio change de signe.  
        Il est utilis√© pour diff√©rencier les sons vocaux et non vocaux, ou les signaux bruyants et propres.
        """)

        if y.ndim == 2:
            # Stereo audio: y has shape (2, n_samples), process each channel separately       
            # Compute Zero Crossing Rate for both channels (left and right)
            zcr_left = librosa.feature.zero_crossing_rate(y=y[0])
            zcr_right = librosa.feature.zero_crossing_rate(y=y[1])

            # Plot Zero Crossing Rate for Left and Right Channels
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

            # Left channel plot
            ax1.plot(librosa.times_like(zcr_left), zcr_left[0], color='b')
            ax1.set_title("Zero Crossing Rate - Left Channel", fontsize=14)
            ax1.set_xlabel("Temps (s)")
            ax1.set_ylabel("Zero Crossing Rate")
            
            # Right channel plot
            ax2.plot(librosa.times_like(zcr_right), zcr_right[0], color='r')
            ax2.set_title("Zero Crossing Rate - Right Channel", fontsize=14)
            ax2.set_xlabel("Temps (s)")
            ax2.set_ylabel("Zero Crossing Rate")

            # Display the plots in Streamlit
            st.pyplot(fig)

        else:
            # Compute Zero Crossing Rate for mono audio
            zcr = librosa.feature.zero_crossing_rate(y=y)

            # Plot Zero Crossing Rate
            fig, ax = plt.subplots(figsize=(10, 4))
            ax.plot(librosa.times_like(zcr), zcr[0], color='g')
            ax.set_title("Zero Crossing Rate", fontsize=14)
            ax.set_xlabel("Temps (s)")
            ax.set_ylabel("Zero Crossing Rate")
            
            # Display the plot in Streamlit
            st.pyplot(fig)
        
        st.markdown("### Visualisation du Centro√Øde Spectral")
        st.markdown("""
        Le centro√Øde spectral indique la "brillance" d'un son. Un centro√Øde plus √©lev√© signifie que l'√©nergie est davantage concentr√©e dans les hautes fr√©quences.
        """)
        if y.ndim == 2:
            y_mono = np.mean(y, axis=0)
        else:
            y_mono = y

        # Compute Spectral Centroid
        spectral_centroid = librosa.feature.spectral_centroid(y=y_mono, sr=sr)

        # Time axis for plotting
        times = librosa.times_like(spectral_centroid)

        # Plotting
        fig, ax = plt.subplots(figsize=(10, 4))
        ax.plot(times, spectral_centroid[0], color='m')
        ax.set_title("Spectral Centroid Over Time")
        ax.set_xlabel("Temps (s)")
        ax.set_ylabel("Hz")
        st.pyplot(fig)


    
        

        st.markdown("### S√©paration des Sources Harmoniques et Percussives")
        st.markdown("""
        - La **composante harmonique** contient les sons avec hauteur d√©finie (par exemple, voix, instruments).
        - La **composante percussive** contient les transitoires et les battements (par exemple, percussions).
        """)
        st.markdown("""
        Ce graphique superpose les composantes harmoniques (bleu) et percussives (orange) du signal audio.  
        Il est utile pour visualiser comment chaque composante contribue au signal global.
        """)
        if y.ndim == 2:
            y = np.mean(y, axis=0)

        # Separate harmonic and percussive signals
        y_harmonic, y_percussive = librosa.effects.hpss(y)

        # Plot both on the same graph
        fig, ax = plt.subplots(figsize=(10, 4))
        librosa.display.waveshow(y_harmonic, sr=sr, alpha=0.6, label="Harmonic", color="blue", ax=ax)
        librosa.display.waveshow(y_percussive, sr=sr, alpha=0.6, label="Percussive", color="orange", ax=ax)
        ax.set(title="Harmonic and Percussive Signals", xlabel="Temps (s)", ylabel="Amplitude")
        ax.legend()
        st.pyplot(fig)

    
        st.markdown("## Transform√©e Constant-Q (CQT)")
        st.markdown("""
        **üéß Caract√©ristique perceptuelle (CQT) :**  
        La CQT utilise une √©chelle logarithmique des fr√©quences, similaire √† la fa√ßon dont nous percevons la hauteur.  
        Elle est utile pour analyser le contenu harmonique dans la musique.
        """)
        # Perceptual Feature: Constant-Q Transform
        CQT = librosa.amplitude_to_db(np.abs(librosa.cqt(y, sr=sr)), ref=np.max)

        fig_cqt, ax_cqt = plt.subplots(figsize=(10, 4))
        img = librosa.display.specshow(CQT, sr=sr, x_axis='time', y_axis='cqt_note', ax=ax_cqt, cmap='magma')
        ax_cqt.set(title='Constant-Q Transform (CQT)')
        fig_cqt.colorbar(img, ax=ax_cqt, format="%+2.0f dB")
        st.pyplot(fig_cqt)

        st.markdown("### Suivi du Tempo et des Battements")
        st.markdown("""
        Le tempo correspond √† la vitesse des battements dans la musique, mesur√©e en BPM (Battements Par Minute).  
        Les lignes rouges en pointill√©s indiquent les battements estim√©s dans votre audio.
        """)

        if y.ndim == 2:
            y = np.mean(y, axis=0)

        # Estimate tempo and beat frames
        tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)
        beat_times = librosa.frames_to_time(beat_frames, sr=sr)

        st.write(f"Tempo: {tempo[0]:.2f} BPM")

        # st.markdown("#### D√©tection des Battements")
        st.markdown("""
        Cette visualisation montre les **battements** d√©tect√©s dans la forme d'onde, avec des **cercles rouges** autour de chaque battement dans la plage de temps sp√©cifi√©e.  
        Vous pouvez ajuster le **temps de d√©but** et le **temps de fin** pour zoomer sur une section sp√©cifique de l'audio.
        """)
        if y.ndim == 2:
            y = np.mean(y, axis=0)

        # Total duration of the audio
        total_duration = librosa.get_duration(y=y, sr=sr)

        start_time = st.number_input("Temps de d√©but (en secondes) :", min_value=0.0, max_value=total_duration, value=0.0, step=0.1)
        end_time = st.number_input("Temps de fin (en secondes) :", min_value=start_time, max_value=total_duration, value=total_duration, step=0.1)        
        end_time = min(end_time, total_duration)  # Ensure end time does not exceed total duration
        start_sample = int(start_time * sr)
        end_sample = int((end_time+1) * sr)
        y_cut = y[start_sample:end_sample]

        onset_env = librosa.onset.onset_strength(y=y_cut, sr=sr)
        peaks, _ = find_peaks(onset_env, height=0.1)  # Adjustable height parameter

        fig, ax = plt.subplots(figsize=(10, 4))

        librosa.display.waveshow(y_cut, sr=sr, ax=ax)

        for peak in peaks:
            peak_time = librosa.frames_to_time(peak, sr=sr)  # Convert frame to time
            if start_time <= peak_time <= end_time:  # Ensure beat is within the time range
                ax.plot(peak_time, y_cut[peak], 'ro', markersize=8)  # Red dot at the beat position
                ax.add_patch(plt.Circle((peak_time, y_cut[peak]), radius=0.02, color='r', fill=False, linewidth=2))  # Circle around the beat

        # Set labels and title
        ax.set(title="Forme d'onde avec les battements mis en √©vidence", xlabel="Temps (s)", ylabel="Amplitude")
        # Adjust the x-axis to start from start_time and end at end_time
        ax.set_xlim(start_time, end_time)

        # Show the plot
        st.pyplot(fig)
        

    # ------------------------------
    # Prediction Section (Triggered from Sidebar)
    # ------------------------------
    if do_predictions:
        with st.spinner("Processing predictions..."):
            # Load the model (ensure xgb_model2.pkl is in the same directory)
            with open("results/xgb_model6.pkl", "rb") as model_file:
                model = joblib.load(model_file)

            # Overall Prediction on full track
            features_array = build_features_array(full_features)
            overall_probabilities = model.predict_proba(features_array, validate_features=False)[0]
            class_labels = model.classes_

            st.subheader("üéØ Pr√©visions g√©n√©rales")
            overall_df = pd.DataFrame({
                "Genre": class_labels,
                "Probability (%)": overall_probabilities * 100
            }).sort_values(by="Probability (%)", ascending=False)
            st.dataframe(overall_df)

            # ------------------------------
            # Genre Prediction Over Time
            # ------------------------------
            st.subheader("‚è±Ô∏è Pr√©dictions des Genres au Fil du Temps")
            st.write(f"La piste est divis√©e en segments de {chunk_duration} secondes, et les pr√©dictions sont trac√©es au fil du temps.")            
            num_samples_per_chunk = int(chunk_duration * sr)
            num_chunks = int(math.ceil(len(audio_mono) / num_samples_per_chunk))

            time_points = [] 
            predictions_list = []  

            for i in range(num_chunks):
                start = i * num_samples_per_chunk
                end = min((i + 1) * num_samples_per_chunk, len(audio_mono))
                chunk = audio_mono[start:end]
                if len(chunk) < num_samples_per_chunk:
                    pad_width = num_samples_per_chunk - len(chunk)
                    chunk = np.pad(chunk, (0, pad_width), 'constant')
                chunk_features = analyze_audio(chunk, sr)
                features_array_chunk = build_features_array(chunk_features)
                chunk_probabilities = model.predict_proba(features_array_chunk, validate_features=False)[0]
                predictions_list.append(chunk_probabilities)
                midpoint = (start + end) / 2 / sr  # midpoint in seconds
                time_points.append(midpoint)

            predictions_df = pd.DataFrame(predictions_list, columns=class_labels)
            predictions_df["Time (s)"] = time_points

            # Plot predictions over time for each genre
            fig3, ax3 = plt.subplots(figsize=(12, 6))
            for genre in class_labels:
                ax3.plot(predictions_df["Time (s)"], predictions_df[genre] * 100,
                         marker='o', label=genre)
            ax3.set_xlabel("Temps (s)")
            ax3.set_ylabel("Probabilit√© (%)")
            ax3.set_title("Pr√©dictions des Genres au Fil du Temps")
            ax3.legend(loc="upper right", bbox_to_anchor=(1.15, 1))
            st.pyplot(fig3)

            st.subheader("Donn√©es de Pr√©diction (par segment)")
            st.dataframe(predictions_df)

    if os.path.exists(tmp_path):
        os.unlink(tmp_path)
else:
    st.info("‚¨ÜÔ∏è Veuillez t√©l√©charger un fichier audio depuis la barre lat√©rale pour commencer l'analyse.")